````markdown
# sas2spark

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Build Status](https://img.shields.io/badge/build-passing-brightgreen)](https://github.com/KevinHe/sas2spark) <!-- Placeholder -->
[![PyPI version](https://badge.fury.io/py/sas2spark.svg)](https://badge.fury.io/py/sas2spark) <!-- Placeholder if published -->

**Translate SAS code to Apache Spark (PySpark/Scala) to accelerate migration.**

## Overview

`sas2spark` is a utility designed to assist developers and data engineers in migrating legacy SAS workloads to modern distributed computing frameworks like Apache Spark. It parses SAS scripts (`.sas` files) and attempts to generate equivalent PySpark or Scala code, focusing on common data manipulation tasks found in DATA steps and PROC SQL.

## Motivation

Migrating large codebases from SAS to Spark can be a complex, time-consuming, and error-prone process. While a direct, perfect translation is often impossible due to fundamental differences between the platforms, `sas2spark` aims to automate the conversion of common patterns, reducing manual effort and providing a solid starting point for the migration. This allows teams to focus on the more complex logic, validation, and optimization aspects of the migration.

## Features

- **SAS Parsing:** Parses common SAS syntax elements including DATA steps, PROC SQL, PROC SORT, etc.
- **Code Generation:** Translates SAS logic into corresponding PySpark or Scala Spark DataFrame operations.
- **Function Mapping:** Attempts to map common SAS functions to their Spark equivalents.
- **Control Flow:** Handles basic SAS control flow like `IF/THEN/ELSE`.
- **Readability:** Generated Spark code aims to be readable and maintainable.
- **Extensibility:** (Future Goal) Designed to be extensible for supporting more SAS procedures and functions.

## Installation

### Using pip (Recommended)

```bash
pip install sas2spark
```
````

_(Note: This assumes the package is published on PyPI. If not, use the source installation method.)_

### From Source

```bash
git clone https://github.com/KevinHe/sas2spark.git
cd sas2spark
pip install .
# Or for development
# pip install -e .
```

## Usage

### Command Line Interface (CLI)

The primary way to use `sas2spark` is via its command-line interface.

```bash
sas2spark --input <path_to_sas_file.sas> --output <path_to_output_spark_script.py> --dialect pyspark
```

**Options:**

- `--input`, `-i`: Path to the input SAS file. (Required)
- `--output`, `-o`: Path to the output Spark script file. (Required)
- `--dialect`, `-d`: The target Spark dialect. Options: `pyspark` (default), `scala`.
- `--overwrite`: Allow overwriting the output file if it exists.
- `--verbose`, `-v`: Enable verbose logging.

**Example:**

```bash
sas2spark -i ./my_etl.sas -o ./my_etl_spark.py -d pyspark --overwrite
```

### As a Library (Planned)

```python
# Planned API - Subject to change
from sas2spark import SASParser, PySparkConverter

sas_code = """
DATA work.output;
  SET work.input;
  new_var = old_var * 10;
  IF category = 'A' THEN flag = 1; ELSE flag = 0;
RUN;
"""

parser = SASParser()
ast = parser.parse(sas_code)

converter = PySparkConverter()
spark_code = converter.convert(ast)

print(spark_code)
```

## Examples

### Example 1: Simple DATA Step

**SAS Code (`input.sas`):**

```sas
DATA work.output_data;
  SET work.input_data (KEEP=id value category);
  RENAME value=metric;

  new_metric = metric * 100;
  IF category = 'X' THEN priority = 1;
  ELSE priority = 0;

  DROP category;
RUN;
```

**Generated PySpark Code (`output.py`):**

```python
# Generated by sas2spark

from pyspark.sql import SparkSession
from pyspark.sql import functions as F

spark = SparkSession.builder.appName("sas2spark_conversion").getOrCreate()

# Assuming 'input_data' DataFrame is loaded, e.g.:
# input_data = spark.table("work.input_data")
# Or:
# input_data = spark.read.parquet("/path/to/input_data")

# SET work.input_data (KEEP=id value category);
# RENAME value=metric;
df = input_data.select("id", F.col("value").alias("metric"), "category")

# new_metric = metric * 100;
df = df.withColumn("new_metric", F.col("metric") * 100)

# IF category = 'X' THEN priority = 1; ELSE priority = 0;
df = df.withColumn("priority",
  F.when(F.col("category") == 'X', 1)
  .otherwise(0)
)

# DROP category;
df = df.drop("category")

# Output DataFrame 'df' corresponds to work.output_data
# You might want to write it out:
# df.write.mode("overwrite").saveAsTable("work.output_data")
# Or:
# df.write.parquet("/path/to/output_data")

df.show() # For demonstration

```

### Example 2: Simple PROC SQL

**SAS Code (`input.sas`):**

```sas
PROC SQL;
  CREATE TABLE work.summary AS
  SELECT
    region,
    SUM(sales) AS total_sales,
    COUNT(DISTINCT customer_id) AS unique_customers
  FROM work.transactions
  WHERE sales > 0
  GROUP BY region
  HAVING COUNT(*) > 10
  ORDER BY total_sales DESC;
QUIT;
```

**Generated PySpark Code (`output.py`):**

```python
# Generated by sas2spark

from pyspark.sql import SparkSession
from pyspark.sql import functions as F

spark = SparkSession.builder.appName("sas2spark_conversion").getOrCreate()

# Assuming 'transactions' DataFrame is loaded
# transactions = spark.table("work.transactions")

# PROC SQL equivalent
summary = transactions \
  .where(F.col("sales") > 0) \
  .groupBy("region") \
  .agg(
    F.sum("sales").alias("total_sales"),
    F.countDistinct("customer_id").alias("unique_customers"),
    F.count("*").alias("_count_for_having") # Temporary column for HAVING
  ) \
  .where(F.col("_count_for_having") > 10) \
  .drop("_count_for_having") \
  .orderBy(F.col("total_sales").desc())

# Output DataFrame 'summary' corresponds to work.summary
# summary.write.mode("overwrite").saveAsTable("work.summary")
summary.show() # For demonstration
```

## Supported SAS Features (Partial List)

- **DATA Step:**
- `SET` (single dataset)
- `MERGE` (basic joins, requires `BY` statement)
- `BY`
- `KEEP`, `DROP` (variables)
- `RENAME`
- Assignment statements
- `IF/THEN/ELSE`
- Basic arithmetic operations
- Some common functions (e.g., `SUM`, `MEAN`, `LENGTH`, `SUBSTR`, `INPUT`, `PUT` - mapping may vary)
- **PROC SQL:**
- `SELECT` (including `DISTINCT`)
- `FROM` (single table or basic joins)
- `WHERE`
- `GROUP BY`
- `HAVING`
- `ORDER BY`
- `CREATE TABLE AS SELECT`
- Basic aggregate functions (`SUM`, `COUNT`, `AVG`, `MIN`, `MAX`)
- **Procedures:**
- `PROC SORT`: Maps to `DataFrame.orderBy()`

## Limitations

- **Complex Macros:** SAS macro processing is highly complex and dynamic. This tool does not expand or translate complex macros. Simple macro variables might be substituted in some cases.
- **Unsupported PROCs:** Many SAS procedures (especially statistical, graphical, or specialized ones like `PROC REPORT`, `PROC TABULATE`, `PROC GLM`, `PROC REG`, `SAS/GRAPH`, etc.) are not supported.
- **Complex DATA Step Logic:** Advanced DATA step features like arrays, hash objects, `RETAIN`, complex loops (`DO OVER`, `DO WHILE`), `PUT`/`INPUT` with complex formats, and direct OS interaction are generally not translated.
- **SAS Options:** Global SAS options (`OPTIONS`) are typically ignored.
- **Implicit Logic:** SAS has implicit behaviors (e.g., automatic type conversions, loop cycling in DATA step) that don't map directly to Spark's explicit nature.
- **Performance:** The generated Spark code is a starting point and may not be optimally performant. Manual optimization is often required.
- **Error Handling:** SAS error handling and logging mechanisms differ significantly from Spark.

**Important:** Always thoroughly review, test, and potentially refactor the generated Spark code. Treat `sas2spark` as an accelerator, not a magic bullet.

## Contributing

Contributions are welcome! Please follow these steps:

1.  **Fork** the repository on GitHub.
2.  **Clone** your fork locally (`git clone git@github.com:YOUR_USERNAME/sas2spark.git`).
3.  Create a **new branch** for your feature or bug fix (`git checkout -b feature/my-new-feature` or `git checkout -b fix/issue-123`).
4.  Make your changes.
5.  Add **tests** for your changes.
6.  Ensure tests pass (`pytest`).
7.  **Lint** your code (e.g., using `flake8`, `black`).
8.  **Commit** your changes (`git commit -am 'Add some feature'`).
9.  **Push** to the branch (`git push origin feature/my-new-feature`).
10. Create a new **Pull Request** on GitHub.

Please provide a clear description of your changes in the pull request.

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

```

```
